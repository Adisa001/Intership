{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e97bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (2409211166.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adisa\\AppData\\Local\\Temp\\ipykernel_19136\\2409211166.py\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    search_query = {'keyword': 'Data Analyst',  'location': 'Bangalore'}\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "search_form = soup.find('form', {'action': '/'})\n",
    "\n",
    "\n",
    "search_query = {'keyword': 'Data Analyst',  'location': 'Bangalore'}\n",
    "\n",
    "for key, value in search_query.items():    \n",
    "    input_field = search_form.find('input', {'name': key})  \n",
    "    input_field['value'] = value\n",
    "\n",
    "response = requests.post(url, data=search_form.a, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_listings = soup.find_all('article', {'class': 'jobTuple bgWhite br4 mb-8'})\n",
    "job_data = []\n",
    "for job_listing in job_listings[:10]:   \n",
    "    job_title = job_listing.find('a', {'class': 'title fw500 ellipsis'})   \n",
    "    job_location = job_listing.find('li', {'class': 'fleft grey-text br2 placeHolderLi location'})  \n",
    "    company_name = job_listing.find('a', {'class': 'subTitle ellipsis fleft'})   \n",
    "    experience_required = job_listing.find('li', {'class': 'fleft grey-text br2 placeHolderLi experience'})\n",
    "    \n",
    "    job_data.append({        'Job Title': job_title.text.strip(),        'Job Location': job_location.text.strip(),        'Company Name': company_name.text.strip(),        'Experience Required': experience_required.text.strip()    })\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7abe729",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (3042668674.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adisa\\AppData\\Local\\Temp\\ipykernel_19136\\3042668674.py\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    search_query = {    'keyword': 'Data Analyst',    'location': 'Bangalore'}\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "search_form = soup.find('form', {'action': '/'})\n",
    "\n",
    "\n",
    "search_query = {    'keyword': 'Data Analyst',    'location': 'Bangalore'}\n",
    "\n",
    "for key, value in search_query.items():   \n",
    "    input_field = search_form.find('input', {'name': key})  \n",
    "    input_field['value'] = value\n",
    "\n",
    "response = requests.post(url, data=search_form.a, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "job_listings = soup.find_all('article', {'class': 'jobTuple bgWhite br4 mb-8'})\n",
    "\n",
    "job_data = []\n",
    "for job_listing in job_listings[:10]:   \n",
    "    job_title = job_listing.find('a', {'class': 'title fw500 ellipsis'})    \n",
    "    job_location = job_listing.find('li', {'class': 'fleft grey-text br2 placeHolderLi location'})   \n",
    "    company_name = job_listing.find('a', {'class': 'subTitle ellipsis fleft'})    \n",
    "    experience_required = job_listing.find('li', {'class': 'fleft grey-text br2 placeHolderLi experience'})\n",
    "    job_data.append({        'Job Title': job_title.text.strip(),        'Job Location': job_location.text.strip(),        'Company Name': company_name.text.strip(),        'Experience Required': experience_required.text.strip()    })\n",
    "\n",
    "df = pd.DataFrame(job_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e93dcad",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'send_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23548\\1132005173.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msearch_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'qsb-keyword-sugg'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0msearch_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data Scientist'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'send_keys'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "search_box = soup.find('input', {'id': 'qsb-keyword-sugg'})\n",
    "search_box.send_keys('Data Scientist')\n",
    "\n",
    "\n",
    "search_button = soup.find('div', {'class': 'search-btn'})\n",
    "search_button.click()\n",
    "\n",
    "\n",
    "location_filter = soup.find('div', {'class': 'mt-8 chckBoxCont'})\n",
    "location_filter.find_all('label')[1].click()\n",
    "\n",
    "\n",
    "salary_filter = soup.find('div', {'class': 'salaryFilter'})\n",
    "salary_filter.find_all('label')[1].click()\n",
    "\n",
    "\n",
    "job_listings = soup.find_all('article', {'class': 'jobTuple'})\n",
    "\n",
    "\n",
    "job_titles = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "job_locations = []\n",
    "\n",
    "\n",
    "for job in job_listings[:10]:\n",
    "    job_titles.append(job.find('a', {'class': 'title'}).text)\n",
    "    company_names.append(job.find('a', {'class': 'subTitle'}).text)\n",
    "    experience_required.append(job.find('li', {'class': 'fleft grey-text br2 placeHolderLi experience'}).text)\n",
    "    job_locations.append(job.find('li', {'class': 'fleft grey-text br2 placeHolderLi location'}).text)\n",
    "\n",
    "\n",
    "data = {'Job Title': job_titles, 'Company Name': company_names, 'Experience Required': experience_required, 'Job Location': job_locations}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5868529",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23548\\806822100.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0msearch_box\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0msearch_box\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.flipkart.com/'\n",
    "search_query = 'sunglasses'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "search_box = soup.find('input', {'type': 'text'})\n",
    "search_box.send_keys(search_query)\n",
    "\n",
    "\n",
    "search_button = soup.find('button', {'type': 'submit'})\n",
    "search_button.click()\n",
    "\n",
    "\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "\n",
    "while len(brands) < 100:\n",
    "\n",
    "    \n",
    "    listings = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "    \n",
    "    for listing in listings:\n",
    "        brand = listing.find('div', {'class': '_2WkVRV'}).text\n",
    "        description = listing.find('a', {'class': '_2mylT6'}).text\n",
    "        price = listing.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "    \n",
    "    next_button = soup.find('a', {'class': '_1LKTO3'})\n",
    "    if next_button is None:\n",
    "        break\n",
    "    next_button_url = 'https://www.flipkart.com/' + next_button['href']\n",
    "    response = requests.get(next_button_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "data = {'Brand': brands, 'Product Description': descriptions, 'Price': prices}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09517e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Rating, Review Summary, Full Review]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCENY&lid=LSTMOBFWQ6BXGJCENYZXSHRJ&marketplace=FLIPKART'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "\n",
    "while len(ratings) < 100:\n",
    "\n",
    "\n",
    "    listings = soup.find_all('div', {'class': '_27M-vq'})\n",
    "\n",
    "    \n",
    "    for listing in listings:\n",
    "        rating = listing.find('div', {'class': '_3LWZlK _1BLPMq'}).text\n",
    "        review_summary = listing.find('p', {'class': '_2-N8zT'}).text\n",
    "        full_review = listing.find('div', {'class': 't-ZTKy'}).div.div.text\n",
    "        ratings.append(rating)\n",
    "        review_summaries.append(review_summary)\n",
    "        full_reviews.append(full_review)\n",
    "\n",
    "    \n",
    "    next_button = soup.find('a', {'class': '_1LKTO3'})\n",
    "    if next_button is None:\n",
    "        break\n",
    "    next_button_url = 'https://www.flipkart.com/' + next_button['href']\n",
    "    response = requests.get(next_button_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "data = {'Rating': ratings, 'Review Summary': review_summaries, 'Full Review': full_reviews}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc672e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Brand, Description, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&sort=popularity'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "brands = []\n",
    "descriptions = []\n",
    "prices = []\n",
    "\n",
    "\n",
    "while len(brands) < 100:\n",
    "\n",
    "    \n",
    "    listings = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "    \n",
    "    for listing in listings:\n",
    "        brand = listing.find('div', {'class': '_2B_pmu'}).text\n",
    "        description = listing.find('a', {'class': '_2mylT6'}).text\n",
    "        price = listing.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "        brands.append(brand)\n",
    "        descriptions.append(description)\n",
    "        prices.append(price)\n",
    "\n",
    "    \n",
    "    next_button = soup.find('a', {'class': '_1LKTO3'})\n",
    "    if next_button is None:\n",
    "        break\n",
    "    next_button_url = 'https://www.flipkart.com/' + next_button['href']\n",
    "    response = requests.get(next_button_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "data = {'Brand': brands, 'Description': descriptions, 'Price': prices}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084e1c4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1747943451.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adisa\\AppData\\Local\\Temp\\ipykernel_2560\\1747943451.py\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    while count < 100:\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "PATH = \"path/to/chromedriver\"\n",
    "service = Service(PATH)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "search_bar = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_bar.send_keys(\"Laptop\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "time.sleep(2)\n",
    "cpu_type_filter = driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']/preceding-sibling::input\")\n",
    "cpu_type_filter.click()\n",
    "time.sleep(2)\n",
    "\n",
    "data = {'Title': [], 'Ratings': [], 'Price': []}\n",
    "count = 0\n",
    "while count < 100:    \n",
    "    laptop_list = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")    \n",
    "    for laptop in laptop_list:       \n",
    "        try:            \n",
    "            title = laptop.find_element(By.XPATH, \".//h2/a/span\").text       \n",
    "            except:           \n",
    "                title = \"\"        \n",
    "                try:            \n",
    "                    rating = laptop.find_element(By.XPATH, \".//span[@class='a-icon-alt']\")\\ .get_attribute(\"innerHTML\").split()[0]       \n",
    "                    except:            \n",
    "                        rating = \"\"        \n",
    "                        try:            \n",
    "                            price = laptop.find_element(By.XPATH, \".//span[@class='a-price-whole']\")\\    .text.replace(\",\", \"\")        \n",
    "                            except:            \n",
    "                                price = \"\"        \n",
    "                                if title and rating and price:            \n",
    "                                    data['Title'].append(title)            \n",
    "                                    data['Ratings'].append(rating)            \n",
    "                                    data['Price'].append(price)            \n",
    "                                    count += 1            \n",
    "                                    if count == 100:                \n",
    "                                        break    \n",
    "                                        try:        \n",
    "                                            next_button = driver.find_element(By.XPATH, \"//li[@class='a-last']/a\")        \n",
    "                                            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)       \n",
    "                                            next_button.click()        \n",
    "                                            time.sleep(2)    \n",
    "                                            except:        \n",
    "                                                break\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4618e632",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2560\\2724874530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.azquotes.com/top_quotes.html\"\n",
    "page = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "\n",
    "quotes = []\n",
    "authors = []\n",
    "types = []\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) == 3:\n",
    "        quote = cols[0].text.strip()\n",
    "        author = cols[1].text.strip()\n",
    "        quote_type = cols[2].text.strip()\n",
    "        quotes.append(quote)\n",
    "        authors.append(author)\n",
    "        types.append(quote_type)\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Quote': quotes,\n",
    "    'Author': authors,\n",
    "    'Type': types\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df.head(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d5a30f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2560\\2869924384.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mpm_link\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgk_soup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'List of all prime Ministers of India'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "page = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "gk_link = soup.find('a', text='GK')['href']\n",
    "\n",
    "\n",
    "gk_url = url + gk_link\n",
    "gk_page = requests.get(gk_url)\n",
    "\n",
    "\n",
    "gk_soup = BeautifulSoup(gk_page.content, 'html.parser')\n",
    "\n",
    "\n",
    "pm_link = gk_soup.find('a', text='List of all prime Ministers of India')['href']\n",
    "\n",
    "\n",
    "pm_url = url + pm_link\n",
    "pm_page = requests.get(pm_url)\n",
    "\n",
    "\n",
    "pm_soup = BeautifulSoup(pm_page.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = pm_soup.find('table', {'class': 'content-table'})\n",
    "\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "\n",
    "names = []\n",
    "born_dead = []\n",
    "terms_of_office = []\n",
    "remarks = []\n",
    "\n",
    "\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) == 4:\n",
    "        name = cols[0].text.strip()\n",
    "        born_dead_text = cols[1].text.strip()\n",
    "        born_dead_parts = born_dead_text.split(' - ')\n",
    "        born_dead.append(born_dead_text)\n",
    "        names.append(name)\n",
    "        terms_of_office.append(cols[2].text.strip())\n",
    "        remarks.append(cols[3].text.strip())\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Name': names,\n",
    "    'Born-Dead': born_dead,\n",
    "    'Term of Office': terms_of_office,\n",
    "    'Remarks': remarks\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2caba31e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2560\\4059734823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdropdown_menu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"js-dropdown-menu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlist_option\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropdown_menu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mlist_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_option\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlist_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.motor1.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "dropdown_menu = soup.find(\"div\", class_=\"js-dropdown-menu\")\n",
    "list_option = dropdown_menu.find_all(\"a\")[1]\n",
    "list_url = list_option[\"href\"]\n",
    "list_response = requests.get(list_url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(list_response.content, \"html.parser\")\n",
    "expensive_cars_link = soup.find(\"a\", text=\"50 Most Expensive Cars in the World\")\n",
    "expensive_cars_url = expensive_cars_link[\"href\"]\n",
    "expensive_cars_response = requests.get(expensive_cars_url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(expensive_cars_response.content, \"html.parser\")\n",
    "table = soup.find(\"table\")\n",
    "rows = table.find_all(\"tr\")[1:] \n",
    "\n",
    "car_data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    car_name = cols[1].text.strip()\n",
    "    car_price = cols[2].text.strip()\n",
    "    car_data.append((car_name, car_price))\n",
    "\n",
    "df = pd.DataFrame(car_data, columns=[\"Car Name\", \"Price\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba173e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
