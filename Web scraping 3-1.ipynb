{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857664eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: guitars\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "search_query = input(\"Enter the product to search: \")\n",
    "\n",
    "\n",
    "url = f\"https://www.amazon.in/s?k={search_query}&ref=nb_sb_noss_2\"\n",
    "\n",
    "\n",
    "response = requests.get(url).content\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "\n",
    "products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "\n",
    "for product in products:\n",
    "    \n",
    "    name = product.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        price = product.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "        currency = product.find('span', {'class': 'a-price-symbol'}).text.strip()\n",
    "        price = currency + price\n",
    "    except AttributeError:\n",
    "        price = \"Not available\"\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        rating = product.find('span', {'class': 'a-icon-alt'}).text.strip()\n",
    "    except AttributeError:\n",
    "        rating = \"Not available\"\n",
    "    \n",
    "    \n",
    "    print(f\"Product: {name}\")\n",
    "    print(f\"Price: {price}\")\n",
    "    print(f\"Rating: {rating}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e014ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: guitar\n",
      "Data saved to guitar.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "search_query = input(\"Enter the product to search: \")\n",
    "\n",
    "\n",
    "product_data = []\n",
    "\n",
    "\n",
    "max_pages = 3\n",
    "\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "while page_num <= max_pages:\n",
    "    \n",
    "    url = f\"https://www.amazon.in/s?k={search_query}&page={page_num}\"\n",
    "    \n",
    "    \n",
    "    response = requests.get(url).content\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    \n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "    \n",
    "    \n",
    "    for product in products:\n",
    "        \n",
    "        try:\n",
    "            name = product.find('span', {'class': 'a-size-medium'}).text.strip()\n",
    "            url = \"https://www.amazon.in\" + product.find('a', {'class': 'a-link-normal'})['href']\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            brand = product.find('span', {'class': 'a-size-base-plus'}).text.strip()\n",
    "        except AttributeError:\n",
    "            brand = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            price = product.find('span', {'class': 'a-price-whole'}).text.strip()\n",
    "            currency = product.find('span', {'class': 'a-price-symbol'}).text.strip()\n",
    "            price = currency + price\n",
    "        except AttributeError:\n",
    "            price = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            return_policy = product.find('div', {'class': 'a-column a-span2'}).text.strip()\n",
    "        except AttributeError:\n",
    "            return_policy = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            delivery = product.find('div', {'class': 'a-column a-span3'}).text.strip()\n",
    "            delivery = re.sub(r'\\n', '', delivery)\n",
    "        except AttributeError:\n",
    "            delivery = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            availability = product.find('div', {'class': 'a-column a-span7'}).text.strip()\n",
    "        except AttributeError:\n",
    "            availability = \"-\"\n",
    "        \n",
    "        \n",
    "        product_data.append([brand, name, price, return_policy, delivery, availability, url])\n",
    "    \n",
    "    \n",
    "    next_button = soup.find('span', {'class': 's-pagination-item s-pagination-next s-pagination-button'})\n",
    "    if next_button is None:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    page_num += 1\n",
    "\n",
    "\n",
    "df = pd.DataFrame(product_data, columns=['Brand', 'Name', 'Price', 'Return/Exchange', 'Expected Delivery', 'Availability', 'Product URL'])\n",
    "\n",
    "\n",
    "df.to_csv(f\"{search_query}.csv\", index=False)\n",
    "\n",
    "\n",
    "print(f\"Data saved to {search_query}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e194eb3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7652\\122947682.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mimage_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mimage_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"images/{keyword}_{i}.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    571\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         )\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    485\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mscheme\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             raise MissingSchema(\n\u001b[0m\u001b[0;32m    440\u001b[0m                 \u001b[1;34mf\"Invalid URL {url!r}: No scheme supplied. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;34mf\"Perhaps you meant http://{url}?\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL '/images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif': No scheme supplied. Perhaps you meant http:///images/branding/searchlogo/1x/googlelogo_desk_heirloom_color_150x55dp.gif?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "\n",
    "for keyword in keywords:\n",
    "    \n",
    "    url = f\"https://www.google.com/search?q={keyword}&tbm=isch\"\n",
    "    \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    images = soup.find_all('img')\n",
    "    \n",
    "   \n",
    "    for i, image in enumerate(images[:10]):\n",
    "        image_url = image['src']\n",
    "        image_data = requests.get(image_url).content\n",
    "        with open(f\"images/{keyword}_{i}.jpg\", \"wb\") as f:\n",
    "            f.write(image_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df5f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "search_url = 'https://www.flipkart.com/search'\n",
    "params = {'q': input('Enter a smartphone name: '), 'otracker': 'search', 'otracker1': 'search', 'marketplace': 'FLIPKART', 'as-show': 'on', 'sort': ''}\n",
    "response = requests.get(search_url, params=params)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "products = soup.find_all('div', {'class': '_2kHMtA'})\n",
    "\n",
    "\n",
    "brand_list = []\n",
    "name_list = []\n",
    "color_list = []\n",
    "ram_list = []\n",
    "rom_list = []\n",
    "primary_camera_list = []\n",
    "secondary_camera_list = []\n",
    "display_size_list = []\n",
    "battery_capacity_list = []\n",
    "price_list = []\n",
    "url_list = []\n",
    "\n",
    "\n",
    "for product in products:\n",
    "   \n",
    "    title = product.find('div', {'class': '_4rR01T'})\n",
    "    brand_name, smartphone_name = title.text.split(' ', 1)\n",
    "    \n",
    "    \n",
    "    color = product.find('div', {'class': '_3IrTcX'})\n",
    "    color_text = color.text if color else '-'\n",
    "    \n",
    "    \n",
    "    specs = product.find('ul', {'class': '_1xgFaf'})\n",
    "    spec_list = specs.text.split('|')\n",
    "    ram = '-'\n",
    "    rom = '-'\n",
    "    for spec in spec_list:\n",
    "        if 'RAM' in spec:\n",
    "            ram = spec.strip()\n",
    "        elif 'ROM' in spec:\n",
    "            rom = spec.strip()\n",
    "    \n",
    "    \n",
    "    camera = product.find('ul', {'class': '_1xgFaf'}, text='Camera')\n",
    "    camera_list = camera.parent.text.split('|')\n",
    "    primary_camera = '-'\n",
    "    secondary_camera = '-'\n",
    "    for c in camera_list:\n",
    "        if 'MP' in c:\n",
    "            if primary_camera == '-':\n",
    "                primary_camera = c.strip()\n",
    "            else:\n",
    "                secondary_camera = c.strip()\n",
    "    \n",
    "    \n",
    "    display = product.find('ul', {'class': '_1xgFaf'}, text='Display')\n",
    "    display_size = display.parent.text.split('|')[0].strip() if display else '-'\n",
    "    \n",
    "    \n",
    "    battery = product.find('ul', {'class': '_1xgFaf'}, text='Battery')\n",
    "    battery_capacity = battery.parent.text.split('|')[0].strip() if battery else '-'\n",
    "    \n",
    "    \n",
    "    price = product.find('div', {'class': '_30jeq3 _1_WHN1'})\n",
    "    url = product.find('a', {'class': '_1fQZEK'})['href']\n",
    "    \n",
    "    \n",
    "    brand_list.append(brand_name)\n",
    "    name_list.append(smartphone_name)\n",
    "    color_list.append(color_text)\n",
    "    ram_list.append(ram)\n",
    "    rom_list.append(rom)\n",
    "    primary_camera_list.append(primary_camera)\n",
    "    secondary_camera_list.append(secondary_camera)\n",
    "    display_size_list.append(display_size)\n",
    "    battery_capacity_list.append(battery_capacity)\n",
    "    price_list.append(price.text if price else '-')\n",
    "    url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b84253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city name: Lagos\n",
      "Error: Could not find the geospatial coordinates.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "\n",
    "\n",
    "city_name = input('Enter the city name: ')\n",
    "\n",
    "\n",
    "params = {'address': city_name,'key': 'YOUR_API_KEY_HERE'}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "\n",
    "if data['status'] == 'OK':\n",
    "    \n",
    "    lat = data['results'][0]['geometry']['location']['lat']\n",
    "    lng = data['results'][0]['geometry']['location']['lng']\n",
    "    \n",
    "    \n",
    "    print(f'The geospatial coordinates of {city_name} are: ({lat}, {lng})')\n",
    "else:\n",
    "    print('Error: Could not find the geospatial coordinates.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "174c86a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Sr. No., Date (dd/mm/yyyy), Startup Name, Industry / Vertical, Sub-Vertical, City / Location, Investors' Name, Investment Type, Amount (In USD)]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = 'https://trak.in/india-startup-funding-investment-2015/'\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find_all('table')[6]\n",
    "\n",
    "\n",
    "headers = [header.text.strip() for header in table.find_all('th')]\n",
    "\n",
    "\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    rows.append([data.text.strip() for data in row.find_all('td')])\n",
    "\n",
    "\n",
    "q2_rows = []\n",
    "for row in rows:\n",
    "    if row and row[0].startswith('Q2') and '2021' in row[0]:\n",
    "        q2_rows.append(row)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(q2_rows, columns=headers)\n",
    "\n",
    "\n",
    "df.to_csv('funding_deals_q2_2021.csv', index=False)\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332d0320",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (4228678970.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adisa\\AppData\\Local\\Temp\\ipykernel_17496\\4228678970.py\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    for spec in soup.find_all(\"div\", {\"class\": \"Spcs-rght\"}):\u001b[0m\n\u001b[1;37m                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "laptop_names = [laptop.text for laptop in soup.find_all(\"div\", {\"class\": \"TopNumbeHeading\"})]\n",
    "\n",
    "specifications = [spec.text.strip().replace('\\n\\n\\n\\n\\n\\n', '\\n') for spec in soup.find_all(\"div\", {\"class\": \"Spcs-lft\"})]\n",
    "\n",
    "spec_values = []\n",
    "for spec in soup.find_all(\"div\", {\"class\": \"Spcs-rght\"}):  \n",
    "    value = spec.text.strip().replace('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', ',').replace('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', ',').replace('\\n\\n\\n\\n\\n\\n', ',').replace('\\n\\n\\n', ',').replace('\\n', ',')  \n",
    "    spec_values.append(value)\n",
    "\n",
    "laptops = []\n",
    "for i in range(len(laptop_names)):  \n",
    "    laptop = {}   \n",
    "    laptop[\"Name\"] = laptop_names[i]  \n",
    "    specs = specifications[i].split(\"\\n\")    \n",
    "    values = spec_values[i].split(\",\")   \n",
    "    for j in range(len(specs)):     \n",
    "        laptop[specs[j]] = values[j]   \n",
    "        laptops.append(laptop)\n",
    "\n",
    "df = pd.DataFrame(laptops)\n",
    "\n",
    "df.to_csv(\"best_gaming_laptops.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769ae322",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17496\\276388522.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "\n",
    "ranks = []\n",
    "names = []\n",
    "net_worths = []\n",
    "ages = []\n",
    "citizenships = []\n",
    "sources = []\n",
    "industries = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    \n",
    "    rank = row.find_all('td')[0].text.strip()\n",
    "    name = row.find_all('td')[1].text.strip()\n",
    "    net_worth = row.find_all('td')[2].text.strip()\n",
    "    age = row.find_all('td')[3].text.strip()\n",
    "    citizenship = row.find_all('td')[4].text.strip()\n",
    "    source = row.find_all('td')[5].text.strip()\n",
    "    industry = row.find_all('td')[6].text.strip()\n",
    "\n",
    "    \n",
    "    ranks.append(rank)\n",
    "    names.append(name)\n",
    "    net_worths.append(net_worth)\n",
    "    ages.append(age)\n",
    "    citizenships.append(citizenship)\n",
    "    sources.append(source)\n",
    "    industries.append(industry)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'Rank': ranks,'Name': names,'Net Worth': net_worths,'Age': ages,'Citizenship': citizenships,'Source': sources,'Industry': industries})\n",
    "\n",
    "\n",
    "df.to_csv('billionaires.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e589cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-auth\n",
      "  Downloading google_auth-2.18.0-py2.py3-none-any.whl (178 kB)\n",
      "     ------------------------------------ 178.9/178.9 kB 269.8 kB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth-httplib2\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.86.0-py2.py3-none-any.whl (11.3 MB)\n",
      "     ---------------------------------------- 11.3/11.3 MB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth) (1.26.11)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting httplib2>=0.15.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.9/96.9 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.3/120.3 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.23.0-cp39-cp39-win_amd64.whl (422 kB)\n",
      "     -------------------------------------- 422.5/422.5 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "     -------------------------------------- 223.6/223.6 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2>=0.15.0->google-auth-httplib2) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.3)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, oauthlib, httplib2, cachetools, requests-oauthlib, googleapis-common-protos, google-auth, google-auth-oauthlib, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.3.0 google-api-core-2.11.0 google-api-python-client-2.86.0 google-auth-2.18.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-1.0.0 googleapis-common-protos-1.59.0 httplib2-0.22.0 oauthlib-3.2.2 protobuf-4.23.0 requests-oauthlib-1.3.1 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\adisa\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\adisa\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ecc4ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=%3Cyour+video+ID%3E&maxResults=100&key=%3Cyour+api+key%3E&alt=json returned \"API key not valid. Please pass a valid API key.\". Details: \"[{'message': 'API key not valid. Please pass a valid API key.', 'domain': 'global', 'reason': 'badRequest'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17496\\2907182196.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mnum_comments\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myoutube\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommentThreads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"snippet\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvideoId\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvideo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpageToken\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext_page_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxResults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\googleapiclient\\_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\googleapiclient\\http.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    939\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 400 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=%3Cyour+video+ID%3E&maxResults=100&key=%3Cyour+api+key%3E&alt=json returned \"API key not valid. Please pass a valid API key.\". Details: \"[{'message': 'API key not valid. Please pass a valid API key.', 'domain': 'global', 'reason': 'badRequest'}]\">"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.auth\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "api_key = \"<your api key>\"\n",
    "\n",
    "\n",
    "video_id = \"<your video ID>\"\n",
    "\n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "\n",
    "next_page_token = None\n",
    "comments = []\n",
    "num_comments = 0\n",
    "\n",
    "\n",
    "while num_comments < 500:\n",
    "    \n",
    "    response = youtube.commentThreads().list(part=\"snippet\",videoId=video_id,pageToken=next_page_token,maxResults=100 ).execute()\n",
    "\n",
    "    \n",
    "    for item in response[\"items\"]:\n",
    "        comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "        upvotes = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "        posted_at = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "        comments.append({\"comment\": comment, \"upvotes\": upvotes, \"posted_at\": posted_at})\n",
    "        num_comments += 1\n",
    "\n",
    "    \n",
    "    if \"nextPageToken\" in response:\n",
    "        next_page_token = response[\"nextPageToken\"]\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "for comment in comments:\n",
    "    print(comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c8d2d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1301208812.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\adisa\\AppData\\Local\\Temp\\ipykernel_17496\\1301208812.py\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    for hostel in hostels:\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = 'https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "hostels = soup.find_all('div', class_='property-card')\n",
    "data = []\n",
    "for hostel in hostels:   \n",
    "    name = hostel.find('h2', class_='title').text.strip()    \n",
    "    distance = hostel.find('span', class_='description').text.strip()   \n",
    "    rating = hostel.find('div', class_='score orange').text.strip()    \n",
    "    total_reviews = hostel.find('div', class_='reviews').text.strip()    \n",
    "    overall_reviews = hostel.find('div', class_='keyword').text.strip()    \n",
    "    prices = hostel.find_all('div', class_='price-col')    \n",
    "    privates_from_price = prices[0].find('a').text.strip()   \n",
    "    dorms_from_price = prices[1].find('a').text.strip()    \n",
    "    facilities = [fac.text.strip() \n",
    "                  for fac in hostel.find_all('div', class_='facilities')]   \n",
    "    description = hostel.find('div', class_='description').text.strip()\n",
    "    data.append({        'Name': name,        'Distance from City Centre': distance,        'Rating': rating,        'Total Reviews': total_reviews,        'Overall Reviews': overall_reviews,        'Privates from Price': privates_from_price,        'Dorms from Price': dorms_from_price,        'Facilities': facilities,        'Property Description': description})\n",
    "df = pd.DataFrame(data)df.to_csv('hostels_london.csv', index=False)print('Scraping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f43a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
